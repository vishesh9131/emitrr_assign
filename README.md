# Project's mind 

```
Physician Notetaker
â”œâ”€â”€ 1. Medical NLP Summarization
â”‚   â”œâ”€â”€ Named Entity Recognition (NER)
â”‚   â”‚   â””â”€â”€ Extract: Symptoms, Treatment ,Diagnosis, Prognosis
â”‚   â”œâ”€â”€ Text Summarization
â”‚   â”‚   â””â”€â”€ Convert transcript to structured medical report
â”‚   â””â”€â”€ Keyword Extraction
â”‚       â””â”€â”€ Identify important medical phrases
â”‚
â”œâ”€â”€ 2. Sentiment & Intent Analysis
â”‚   â”œâ”€â”€ Sentiment Classification
â”‚   â”‚   â””â”€â”€ Classify patient sentiment (Anxious, Neutral, Reassured)
â”‚   â””â”€â”€ Intent Detection
â”‚       â””â”€â”€ Identify patient intent (seeking reassurance, reporting symptoms, etc.)
â”‚
â”œâ”€â”€ 3. SOAP Note Generation (Bonus)
â”‚   â”œâ”€â”€ Automated SOAP Note Generation
â”‚   â”œâ”€â”€ Logical Mapping of SOAP sections
â”‚   â””â”€â”€ Text Structuring & Formatting
â”‚
â””â”€â”€ Submission Requirements
    â”œâ”€â”€ GitHub Repository
    â”œâ”€â”€ Live Application
    â””â”€â”€ README with setup instructions, screenshots, and methodology
```

```
prasn_1
~1/
â”œâ”€â”€ app.py                # Main Streamlit application
â”œâ”€â”€ requirements.txt      # Dependencies
â”œâ”€â”€ models/               # For storing models
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ ner.py            # Named Entity Recognition
â”‚   â”œâ”€â”€ summarization.py  # Text Summarization
â”‚   â””â”€â”€ keyword.py        # Keyword Extraction
â””â”€â”€ README.md
```

# ğŸ“Š Model Performance Report

## ğŸ” Overview
This document provides an interactive comparison of three models:
- **Rule-Based Model**
- **BioBERT Model**
- **BERT_CONLL03 Model**

### **ğŸ† Best Performing Model:**
- The **Rule-Based Model** achieved the highest overall accuracy of **0.1470**.

## ğŸ“ˆ Overall Model Accuracy
| Model          | Overall Accuracy | Patient Name Accuracy | Symptoms F1 | Diagnosis Match | Treatment F1 | Status Match | Prognosis Match |
|---------------|-----------------|-----------------------|------------|----------------|-------------|--------------|---------------|
| Rule-Based    | **0.1470**       | **0.7527**           | **0.1387** | **0.0645**     | **0.0339**  | **0.0000**   | **0.0000**    |
| BioBERT      | 0.1014           | 0.6849                | 0.0660     | 0.0323         | 0.0264      | 0.0000       | 0.0000        |
| BERT_CONLL03 | 0.1175           | 0.6774                | 0.0660     | 0.0323         | 0.0264      | 0.0000       | 0.0000        |

## ğŸ“Š Field-by-Field Comparison
| Field                | Rule-Based | BioBERT | BERT_CONLL03 | Best Model |
|----------------------|------------|---------|-------------|------------|
| **Patient Name Accuracy** | **0.7527** | 0.6849  | 0.6774      | **Rule-Based** |
| **Symptoms F1**           | **0.1387** | 0.0660  | 0.0660      | **Rule-Based** |
| **Diagnosis Match**       | **0.0645** | 0.0323  | 0.0323      | **Rule-Based** |
| **Treatment F1**         | **0.0339** | 0.0264  | 0.0264      | **Rule-Based** |
| **Status Match**         | 0.0000    | 0.0000  | 0.0000      | **Tie**       |
| **Prognosis Match**      | 0.0000    | 0.0000  | 0.0000      | **Tie**       |

## ğŸ“Œ Model Comparison by Chunks
This section compares model performance across multiple chunks.

| Chunk     | Rule-Based | BioBERT | BERT_CONLL03 | Best Model |
|-----------|------------|---------|-------------|------------|
| CHUNK_A   | **0.1111** | 0.0370  | 0.0370      | **Rule-Based** |
| CHUNK_B   | **0.4103** | 0.3846  | 0.3846      | **Rule-Based** |
| CHUNK_C   | 0.1667    | 0.1667  | 0.1667      | **Tie** |
| CHUNK_D   | **0.2083** | 0.2000  | 0.2000      | **Rule-Based** |
| CHUNK_E   | 0.2778    | 0.2778  | 0.2778      | **Tie** |
| CHUNK_F   | 0.1667    | 0.0000  | 0.1667      | **Tie** |
| ...       | ...        | ...     | ...         | ... |

### **ğŸ“Œ Summary of Chunk-Wise Best Model Performance:**
- **Rule-Based Model** is best on **26** chunks.
- **BioBERT Model** is best on **16** chunks.
- **BERT_CONLL03 Model** is best on **17** chunks.

## ğŸ¯ Model Performance on Test Chunks
| Model         | Avg. Accuracy on Test Chunks |
|--------------|------------------------------|
| **Rule-Based** | **0.1467** |
| BioBERT      | 0.0300   |
| BERT_CONLL03 | 0.0800   |

### **ğŸ† Best Model on Test Chunks:**
| Model         | Best on # Test Chunks |
|--------------|----------------------|
| **Rule-Based** | **10** (100.0%) |
| BioBERT      | 3 (30.0%)  |
| BERT_CONLL03 | 5 (50.0%)  |

## ğŸ”¬ Name Detection Analysis
| Model         | Name Detection Accuracy |
|--------------|-------------------------|
| Rule-Based    | 0.7667                  |
| **BioBERT**  | **0.8667**               |
| BERT_CONLL03 | 0.7333                   |

## ğŸ“Œ Conclusion
- **Rule-Based Model** performs best overall with the highest accuracy across multiple chunks and test cases.
- **BioBERT Model** excels in **name detection** accuracy.
- **Further improvements** needed in **Status Match & Prognosis Match** as all models score **0.0000** in these fields.

---
ğŸš€ **Explore more insights by running evaluations on new datasets!**

